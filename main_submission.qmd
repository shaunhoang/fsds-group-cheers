---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: CHEER-UP's FSDS Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, CHEER UP Group, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers:

## Brief Group Reflection

| What Went Well | What Was Challenging |
|----------------|----------------------|
| A                  | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

```{python}
import json
import os
import warnings
from urllib.parse import urlparse

import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.graph_objs as go
import seaborn as sns
from requests import get

warnings.simplefilter(action="ignore", category=FutureWarning)
```

```{python}
# Function to download file from remote
# Adapted from: https://www.kaggle.com/code/yotkadata/bivariate-choropleth-map-using-plotly


def cache_data(src: str, dest: str) -> str:
    """
    Create a folder to store file from URL.
    If folder doesn't already exists, then create one, before writing the file.

    src : URL
    dest : location on local drive
    """
    url = urlparse(src)  # We assume that this is some kind of valid URL
    fn = os.path.split(url.path)[-1]  # Extract the filename
    dfn = os.path.join(dest, fn)  # Destination filename as path

    if not os.path.isfile(dfn):
        print(f"{dfn} not found, downloading!")
        path = os.path.split(dest)
        if len(path) >= 1 and path[0] != "":
            os.makedirs(os.path.join(*path), exist_ok=True)
        with open(dfn, "wb") as file:
            response = get(src)
            file.write(response.content)
        print("\tDone downloading...")
    else:
        print(f"Found {dfn} locally!")
    return dfn
```

```{python}
# Function to create bivariate chropleth map

"""
Function to set default variables
"""


def conf_defaults():
    # Define some variables for later use
    conf = {
        "plot_title": "Bivariate choropleth map using Ploty",  # Title text
        "plot_title_size": 20,  # Font size of the title
        "width": 1000,  # Width of the final map container
        "ratio": 0.8,  # Ratio of height to width
        "center_lat": 0,  # Latitude of the center of the map
        "center_lon": 0,  # Longitude of the center of the map
        "map_zoom": 3,  # Zoom factor of the map
        "hover_x_label": "Label x variable",  # Label to appear on hover
        "hover_y_label": "Label y variable",  # Label to appear on hover
        "borders_width": 0.5,  # Width of the geographic entity borders
        "borders_color": "#f8f8f8",  # Color of the geographic entity borders
        # Define settings for the legend
        "top": 1,  # Vertical position of the top right corner (0: bottom, 1: top)
        "right": 1,  # Horizontal position of the top right corner (0: left, 1: right)
        "box_w": 0.04,  # Width of each rectangle
        "box_h": 0.04,  # Height of each rectangle
        "line_color": "#f8f8f8",  # Color of the rectagles' borders
        "line_width": 0,  # Width of the rectagles' borders
        "legend_x_label": "Higher x value",  # x variable label for the legend
        "legend_y_label": "Higher y value",  # y variable label for the legend
        "legend_font_size": 9,  # Legend font size
        "legend_font_color": "#333",  # Legend font color
    }
    # Calculate height
    conf["height"] = conf["width"] * conf["ratio"]

    return conf


"""
Function to recalculate values in case width is changed
"""


def recalc_vars(new_width, variables, conf=conf_defaults()):
    # Calculate the factor of the changed width
    factor = new_width / 1000

    # Apply factor to all variables that have been passed to the function
    for var in variables:
        if var == "map_zoom":
            # Calculate the zoom factor
            # Mapbox zoom is based on a log scale. map_zoom needs to be set
            # to value ideal for our map at 1000px.
            # So factor = 2 ^ (zoom - map_zoom) and zoom = log(factor) / log(2) + map_zoom
            conf[var] = math.log(factor) / math.log(2) + conf[var]
        else:
            conf[var] = conf[var] * factor

    return conf


"""
Function that assigns a value (x) to one of three bins (0, 1, 2).
The break points for the bins can be defined by break_1 and break_2.
"""


def set_interval_value(x, break_1, break_2):
    if x <= break_1:
        return 0
    elif break_1 < x <= break_2:
        return 1
    else:
        return 2


"""
Function that adds a column 'biv_bins' to the dataframe containing the 
position in the 9-color matrix for the bivariate colors
    
Arguments:
    df: Dataframe
    x: Name of the column containing values of the first variable
    y: Name of the column containing values of the second variable

"""


def prepare_df(df, x="x", y="y"):
    # Check if arguments match all requirements
    if df[x].shape[0] != df[y].shape[0]:
        raise ValueError(
            "ERROR: The list of x and y coordinates must have the same length."
        )

    # Calculate break points at percentiles 33 and 66
    x_breaks = np.percentile(df[x], [33, 66])
    y_breaks = np.percentile(df[y], [33, 66])

    # Assign values of both variables to one of three bins (0, 1, 2)
    x_bins = [
        set_interval_value(value_x, x_breaks[0], x_breaks[1]) for value_x in df[x]
    ]
    y_bins = [
        set_interval_value(value_y, y_breaks[0], y_breaks[1]) for value_y in df[y]
    ]

    # Calculate the position of each x/y value pair in the 9-color matrix of bivariate colors
    df["biv_bins"] = [
        int(value_x + 3 * value_y) for value_x, value_y in zip(x_bins, y_bins)
    ]

    return df


"""
Function to create a color square containig the 9 colors to be used as a legend
"""


def create_legend(fig, colors, conf=conf_defaults()):
    # Reverse the order of colors
    legend_colors = colors[:]
    legend_colors.reverse()

    # Calculate coordinates for all nine rectangles
    coord = []

    # Adapt height to ratio to get squares
    width = conf["box_w"]
    height = conf["box_h"] / conf["ratio"]

    # Start looping through rows and columns to calculate corners the squares
    for row in range(1, 4):
        for col in range(1, 4):
            coord.append(
                {
                    "x0": round(conf["right"] - (col - 1) * width, 4),
                    "y0": round(conf["top"] - (row - 1) * height, 4),
                    "x1": round(conf["right"] - col * width, 4),
                    "y1": round(conf["top"] - row * height, 4),
                }
            )

    # Create shapes (rectangles)
    for i, value in enumerate(coord):
        # Add rectangle
        fig.add_shape(
            go.layout.Shape(
                type="rect",
                fillcolor=legend_colors[i],
                line=dict(
                    color=conf["line_color"],
                    width=conf["line_width"],
                ),
                xref="paper",
                yref="paper",
                xanchor="right",
                yanchor="top",
                x0=coord[i]["x0"],
                y0=coord[i]["y0"],
                x1=coord[i]["x1"],
                y1=coord[i]["y1"],
            )
        )

        # Add text for first variable
        fig.add_annotation(
            xref="paper",
            yref="paper",
            xanchor="left",
            yanchor="top",
            x=coord[8]["x1"],
            y=coord[8]["y1"],
            showarrow=False,
            text=conf["legend_x_label"] + " ðŸ ’",
            font=dict(
                color=conf["legend_font_color"],
                size=conf["legend_font_size"],
            ),
            borderpad=0,
        )

        # Add text for second variable
        fig.add_annotation(
            xref="paper",
            yref="paper",
            xanchor="right",
            yanchor="bottom",
            x=coord[8]["x1"],
            y=coord[8]["y1"],
            showarrow=False,
            text=conf["legend_y_label"] + " ðŸ ’",
            font=dict(
                color=conf["legend_font_color"],
                size=conf["legend_font_size"],
            ),
            textangle=270,
            borderpad=0,
        )

    return fig


"""
Function to create the map

Arguments:
    df: The dataframe that contains all the necessary columns
    colors: List of 9 blended colors
    x: Name of the column that contains values of first variable (defaults to 'x')
    y: Name of the column that contains values of second variable (defaults to 'y')
    ids: Name of the column that contains ids that connect the data to the GeoJSON (defaults to 'id')
    name: Name of the column conatining the geographic entity to be displayed as a description (defaults to 'name')
"""


def create_bivariate_map(
    df, colors, geojson, x="x", y="y", ids="id", name="name", conf=conf_defaults()
):
    if len(colors) != 9:
        raise ValueError(
            "ERROR: The list of bivariate colors must have a length eaqual to 9."
        )

    # Recalculate values if width differs from default
    if not conf["width"] == 1000:
        conf = recalc_vars(
            conf["width"],
            ["height", "plot_title_size", "legend_font_size", "map_zoom"],
            conf,
        )

    # Prepare the dataframe with the necessary information for our bivariate map
    df_plot = prepare_df(df, x, y)

    # Create the figure
    fig = go.Figure(
        go.Choroplethmapbox(
            geojson=geojson,
            locations=df_plot[ids],
            z=df_plot["biv_bins"],
            marker_line_width=0.5,
            colorscale=[
                [0 / 8, colors[0]],
                [1 / 8, colors[1]],
                [2 / 8, colors[2]],
                [3 / 8, colors[3]],
                [4 / 8, colors[4]],
                [5 / 8, colors[5]],
                [6 / 8, colors[6]],
                [7 / 8, colors[7]],
                [8 / 8, colors[8]],
            ],
            customdata=df_plot[
                [name, ids, x, y]
            ],  # Add data to be used in hovertemplate
            hovertemplate="<br>".join(
                [  # Data to be displayed on hover
                    "<b>%{customdata[0]}</b> (ID: %{customdata[1]})",
                    conf["hover_x_label"] + ": %{customdata[2]}",
                    conf["hover_y_label"] + ": %{customdata[3]}",
                    "<extra></extra>",  # Remove secondary information
                ]
            ),
        )
    )

    # Add some more details
    fig.update_layout(
        title=dict(
            text=conf["plot_title"],
            font=dict(
                size=conf["plot_title_size"],
            ),
        ),
        mapbox_style="white-bg",
        width=conf["width"],
        height=conf["height"],
        autosize=True,
        mapbox=dict(
            center=dict(
                lat=conf["center_lat"], lon=conf["center_lon"]
            ),  # Set map center
            zoom=conf["map_zoom"],  # Set zoom
        ),
    )

    fig.update_traces(
        marker_line_width=conf[
            "borders_width"
        ],  # Width of the geographic entity borders
        marker_line_color=conf[
            "borders_color"
        ],  # Color of the geographic entity borders
        showscale=False,  # Hide the colorscale
    )

    # Add the legend
    fig = create_legend(fig, colors, conf)

    return fig


# Define sets of 9 colors to be used
# Order: bottom-left, bottom-center, bottom-right, center-left, center-center, center-right, top-left, top-center, top-right
color_sets = {
    "pink-blue": [
        "#e8e8e8",
        "#ace4e4",
        "#5ac8c8",
        "#dfb0d6",
        "#a5add3",
        "#5698b9",
        "#be64ac",
        "#8c62aa",
        "#3b4994",
    ],
    "teal-red": [
        "#e8e8e8",
        "#e4acac",
        "#c85a5a",
        "#b0d5df",
        "#ad9ea5",
        "#985356",
        "#64acbe",
        "#627f8c",
        "#574249",
    ],
    "blue-orange": [
        "#fef1e4",
        "#fab186",
        "#f3742d",
        "#97d0e7",
        "#b0988c",
        "#ab5f37",
        "#18aee5",
        "#407b8f",
        "#5c473d",
    ],
}
```

```{python}
# Define paths for download
ddir = os.path.join("data", "geo")  # destination directory
spath = "https://github.com/jreades/i2p/blob/master/data/src/"  # source path
spath_db = "https://www.dropbox.com/scl/fi/"  # source path for dropbox
host = "http://orca.casa.ucl.ac.uk"
path = "~jreades/data"
```

```{python}
# Read in airbnb, crime, and lsoa stats files. https://www.dropbox.com/home/casa/casa_fsds_cheers.
# Some files have been cleaned, cropped, turned into gdf and hosted on dropbox to save time later on. See cleandata.ipynb in the Github repo for more details on how these files were created.
# # [PATIENCE] Takes about 4 minutes to download and read in the data

# Crime data filtered for September 2023, within Greater London
crime = gpd.read_file(cache_data(spath_db + "ewc9skqcfr930whzcjadd/crime.gpkg?rlkey=xjve2mmmmt5h3allsa2zpabya&dl=1",ddir) , driver="gpkg", low_memory=False)

# Airbnb listing data scraped on September 2023, filtered within Greater London
listings = gpd.read_parquet(cache_data(f"{host}/{path}/2023-09-06-listings.geoparquet", ddir))
listings = listings.to_crs(epsg=27700)

# LSOA (London) data concatenated from multiple datasets from London Datastore,
lsoa_stat = pd.read_csv(cache_data(spath_db + "8zc0g09rlf1yhj6j7yn6n/lsoa_full.csv?rlkey=yopr4hv017rr3iv4e2zus0ifz&dl=1",ddir), low_memory=False)

print("Done.")
```

```{python}
# Read in shapefiles (2 mins)
water = gpd.read_file(cache_data(spath + "Water.gpkg?raw=true", ddir))
green = gpd.read_file(cache_data(spath + "Greenspace.gpkg?raw=true", ddir))
boros = gpd.read_file(cache_data(spath + "Boroughs.gpkg?raw=true", ddir))
lsoa = gpd.read_file(
    cache_data(
        spath_db
        + "u367zlorie8vuluugy2fr/lsoa_london.gpkg?rlkey=rc7rdnlfdmzfgy5q7ujz9pnwj&dl=1",
        ddir,
    )
)
print("Done.")

# Check CRS for all geo dataframes
print(water.crs, green.crs, boros.crs, lsoa.crs, crime.crs, listings.crs)
# All dataframes are in EPSG:27700
```

# Response to Questions

## 1. Who collected the data?

Inside Airbnb was founded by Murray Cox who conceived the project, and keeps updating and analyzing Airbnb data (*About Inside Airbnb*, no date).

## 2. Why did they collect it?

Murray Cox and his partners aim to make Airbnb's data more transparent and accessible to the public. In addition to making visual dashboards of different cities, they also wrote many reports based on these data. For example, they criticized Airbnbâ€™s New York data as misleading (Cox and Slee, 2016).

For consumers, they can understand market trends more easily, which help them make more informed decisions and choose accommodation that fits their needs and budget; For landlords, transparency of information will promote fair competition, and maintain the stability of short-term rental market prices; For academics, they can use these data to explore how Airbnb relates to many factors in urban development; For the government, they can understand the current situation of the short-term rental market and try to control the phenomenon that is not conducive to the stable development of the city by establishing more supervision regulation.

## 3. How was the data collected?

Inside Airbnb uses web scraping scripts to extract publicly available information from Airbnb's website on a quarterly basis. These scripts navigate the Airbnb site, accessing various pages to retrieve data such as listing titles, descriptions, hosts, pricing, and more. The collected data is then verified, cleaned, and made available on the Inside Airbnb website for users to explore online or download for analysis.

In addition to providing a snapshot, Inside Airbnb conducts in-depth analyses using this raw data to derive more insightful information for users. For instance, the platform employs a proprietary "San Francisco Model" to estimate the frequency with which an Airbnb listing is rented out and to approximate a listing's income.

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

1.  Pricing and availability among others are extremely dynamic attributes set by hosts that cannot be captured accurately with web-scraping, which only reflects a snapshot of the website at one specific moment in time.
2.  Susceptible to changes in the structure of the website.
3.  Web scraping only accesses publicly available listings.
4.  Too frequent web scraping may affect site performance which in turns affect the data scraped
5.  Has to ethical and legal implications
6.  Data capture can only capture real-time AirBnb data, and long-term comparative analysis requires data collection in advance

## 5. What ethical considerations does the use of this data raise?

First, Airbnb is a profitable company, making their data public without interfering with normal operations makes it easier to regulate them. But as landlords and renters, they have the right to request that their personal information not be made public elsewhere. However, it is not possible to ask everyoneâ€™s consent when the data is crawled. Therefore, using this data may be needed to handle sensitive personal information. For example, this could have implications for home privacy and home security.

Second, as the data is simply extracted and shown, users can only see the surface and cannot understand the context and logic of it. It may cause inaccuracies usage of data, which can be misused and misguide public opinion (Dâ€™Ignazio and Klein, 2020). For example, the rental activities of a particular group may be negatively analyzed, which will reinforce the stereotype of this group and then trigger discrimination and antagonism mood of society.

Third, data settings are always influenced by power (Dâ€™Ignazio and Klein, 2020). Airbnb often leaves out reviews that arenâ€™t friendly to properties and focuses on those that are profitable, such as setting its recommendation algorithm to favor partners. Therefore, users need to be fair when using such data.

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Listing types suggest about the nature of Airbnb lets in London?

#### Background and research question
Airbnb provides indispensable accommodation for the development of tourism. As a short-term rental platform, the mobility of tenants is one of its major characteristics. Mobility is a factor in crime (Mburu and Helbich, 2016), so recently there has been increasing concern about whether Airbnb, which brings high mobility to the local community, could be linked to an increase or decrease in crime. For example, Xu, Pennington-Gray and Kim (2019) studied the relationship between Airbnb density and crime in Florida and found that certain types of listings do have a significant impact on crime. Therefore, we try to explore whether Airbnb in London could also have an impact on crime based on Lower Layer Super Output Areas (LSOAs) level. The reason why we choose LSOA level as this allows for a more detailed examination of localized patterns and variations within a borough. Different neighborhoods within a borough can exhibit distinct characteristics that might be overlooked in a broader borough-level analysis as crime rates and housing patterns can vary significantly from one neighborhood to another. Finally, our research question is: Whether the type of Airbnb listing have an impact on crime in the neighbourhood? 

#### Data source
The Airbnb dataset is from the September 2023 updated version provided by Inside Airbnb. The crime dataset is from data.police.uk and has the same time dimension as Airbnbâ€™s. The spatial partitioning dimension in this study is LSOA and is from the Office of National Statistics website. 

#### Defining the data
In order to examine the nature of airbnb and crime relationship, we want to first establish the parameters to focus on a subset of the datasets:ï¼ˆè§£é‡Šæ¯ä¸ªå˜é‡çš„å®šä¹‰ï¼‰

As Xu, Pennington-Gray, and Kim (2019) mentioned different types of listings have different impacts, we refer to their classification standards to divide Airbnb housing into private rooms, shared rooms and entire homes, which will be treated as independent variables. When preprocessing Airbnb listings data, we excluded hotel rooms, which are concentrated rather than spread across different residential buildings and have less of an impact on local neighbors. It is worth mentioning that we trim listing price outliers to control for potentially bogus listings. 

```{python}
# Clean listings data: outliers, and missing values, split into 3 types by property type
# Omit rows with Hotel room
listings = listings[~listings["room_type"].isin(["Hotel room"])]

# Transform price to float
if listings.price.dtype == "O":
    listings.price = (
        listings.price.str.replace("$", "").str.replace(",", "").astype(float).round(2)
    )
```

```{python}
# Trim outliers in terms of price based on interquartile range, because they are most likely they are not real listings
q1 = listings.price.quantile(0.25)
q3 = listings.price.quantile(0.75)
iqr = q3 - q1
listings = listings[
    (listings.price >= q1 - 1.5 * iqr) & (listings.price <= q3 + 1.5 * iqr)
]
listings.price.plot.box()
```

```{python}
# listings.plot(column='room_type', cmap='plasma', markersize=.5, alpha=0.15, figsize=(10,7))
```

As for crime data, as we want to make recommendations more specific to different types and levels of crime, we classify crime into the following types (Xu, Pennington-Gray and Kim, 2019; Flatley, 2016):
- Public order: public order, drugs, possession of weapons, anti-social behaviour.
- Violent (involve force): robbery, violence and sexual offences.
- Property (without force): bicycle theft, burglary, criminal damage and arson, shoplifting, theft from the person, vehicle cri
These three types of crime will be treated as dependent variables.e.
e.

```{python}
# Clean crime data: outliers, and missing values, split into 2 types by severity
crime_property = [
    "Burglary",
    "Criminal damage and arson",
    "Theft from the person",
    "Vehicle crime",
    "Shoplifting",
    "Bicycle theft",
]
crime_violent = ["Robbery", "Violence and sexual offences"]
crime_publicorder = [
    "Anti-social behaviour",
    "Public order",
    "Drugs",
    "Possession of weapons",
]
# Need rationalization when writing the report

# Omit rows with Other Crime and Other Theft
crime = crime[~crime["Crime type"].isin(["Other theft", "Other crime"])]
# Create a new column for crime type
crime["Category"] = np.where(
    crime["Crime type"].isin(crime_property),
    "Property crime",
    np.where(
        crime["Crime type"].isin(crime_violent), "Violent crime", "Public Order crime"
    ),
)
```

```{python}
# crime.plot(column='Category', cmap='plasma', markersize=.5, alpha=0.15, figsize=(10,7))
```

#### The spatial visualization of all variables

##### 1. Spatial distribution of each variable
In our spatial analysis of crime and housing patterns across LSOAs, distinct geographic concentrations emerge. Public order crimes are dispersed citywide, with notable pockets in Westminster and Hillingdon. Property crimes concentrate in central London, particularly in Westminster, while violent crimes exhibit a similar pattern, with Westminster and Hillingdon standing out. Concurrently, the distribution of shared rooms reveals widespread dispersion, with heightened concentrations in Hillingdon, Westminster, Tower Hamlets, and Southwark. Private rooms and entire home/apartments exhibit citywide dispersal, with Westminster and Camden featuring prominently in private room concentrations, and the City of London emerging as a hub for entire home/apartments. These spatial insights provide valuable data for informed decision-making in law enforcement, housing policies, and urban planning throughout the diverse neighbourhoods of London.

```{python}
# Plot crime / listings by type
fig, axes = plt.subplots(3, 2, figsize=(18, 18))

lsoa_merge.plot(
    ax=axes[0, 0], column="Public Order crime_count", cmap="Reds", alpha=0.9
)
lsoa_merge.plot(ax=axes[1, 0], column="Property crime_count", cmap="Reds", alpha=0.9)
lsoa_merge.plot(ax=axes[2, 0], column="Violent crime_count", cmap="Reds", alpha=0.9)

axes[0, 0].set_title("Public Order crime by LSOA")
axes[1, 0].set_title("Property crime by LSOA")
axes[2, 0].set_title("Violent crime by LSOA")

lsoa_merge.plot(ax=axes[0, 1], column="Shared room_count", cmap="Blues", alpha=0.9)
lsoa_merge.plot(ax=axes[1, 1], column="Private room_count", cmap="Blues", alpha=0.9)
lsoa_merge.plot(ax=axes[2, 1], column="Entire home/apt_count", cmap="Blues", alpha=0.9)

axes[0, 1].set_title("Shared room by LSOA")
axes[1, 1].set_title("Private room by LSOA")
axes[2, 1].set_title("Entire home/apt by LSOA")

for ax in fig.get_axes():
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])

plt.show()
```

##### 2. Relationship between Airbnb listings and Crime:
In order to have a sense of the relationship between different airbnb listing type and crime, we plotted kernel density estimation (KDE) analysis heat map and correlation matrix.

##### a. KDE analysis
By categorising crime types and Airbnb room types, we do kernel density analysis of different types of variables spatially with simple spatial superposition, in order to explore the relationship between the spatial distribution of different room types and different crime types.

The Shared room shows spatial clustering centered on central of  London and radiates mainly in the east-west direction. Figure A, Shared room and three types of crime spatial density overlay can be found, they all overlap in the central of London area, public crime and violent crime spatial density range is greater than shared room. while property crime and spatial density is less than Shared room.

The Private room spatially shows clustering in the north-east direction of central of London, showing a semi-circular ring. Figure B, Private room with crime type spatial overlay in the middle direction, they both overlap in the north-west direction of central of London.

Entire home/apt shows a spatial density that is striped across the central of London area. Figure C, overlaps with the central of London area north of the Thames for each offence type.

```{python}
# KDE plot of listing types vs Crime categories
# PATIENCE: It takes 3-4 minutes to run

fig, axes = plt.subplots(3, 3, figsize=(32, 24))

# Base
for ax in fig.get_axes():
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])
    boros.plot(edgecolor="red", facecolor="None", ax=ax)

# KDE maps
prop_cat = ["Shared room", "Private room", "Entire home/apt"]
crime_cat = ["Public Order crime", "Property crime", "Violent crime"]
# Create thresholds
levels = [0.2, 0.4, 0.6, 0.8, 1]

for i in range(3):
    for j in range(3):
        sns.kdeplot(
            ax=axes[i, j],
            x=crime[crime["Category"] == crime_cat[i]].geometry.x,
            y=crime[crime["Category"] == crime_cat[i]].geometry.y,
            levels=levels,
            fill=True,
            cmap="Reds",
            alpha=0.5,
        )
        sns.kdeplot(
            ax=axes[i, j],
            x=listings[listings.room_type == prop_cat[j]].geometry.x,
            y=listings[listings.room_type == prop_cat[j]].geometry.y,
            levels=levels,
            fill=True,
            cmap="Blues",
            alpha=0.5,
        )
        axes[i, j].set_title(f"{crime_cat[i]} vs. {prop_cat[j]}")

plt.show()
```

##### b. Correlation analysis
From the correlation matrix, the correlation coefficient between each Airbnb room type and each crime type is greater than 0, which means that they are positively correlated. The correlation between entire home and property crime is the strongest, with a correlation coefficient of 0.509. On the contrary, the correlation between shared room and violent crime is the weakest, with a correlation coefficient of 0.173. Since we now find that all of the independent variables we set are correlated with the dependent variables, we can do further regression analysis to determine whether different types of Airbnb rooms have an impact on different types of crime.

```{python}
lsoa_merge = lsoa

# Spatial crime and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(crime, lsoa_merge)
count_dict = (
    sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
)  # count the values with value counts
lsoa_merge["crime_count"] = lsoa_merge["LSOA21CD"].map(
    count_dict
)  # map it back to lsoa

# Spatial join listings and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(listings, lsoa_merge)
count_dict = (
    sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
)  # count the values with value counts
lsoa_merge["listing_count"] = lsoa_merge["LSOA21CD"].map(
    count_dict
)  # map it back to lsoa
```

```{python}
# Spatial join crime by severity and lsoa, summarise by count
for s in ["Public Order crime", "Property crime", "Violent crime"]:
    sjoin_lsoa = gpd.sjoin(crime[crime["Category"] == s], lsoa_merge)
    count_dict = (
        sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
    )  # count the values with value counts
    lsoa_merge[s + "_count"] = lsoa_merge["LSOA21CD"].map(
        count_dict
    )  # map it back to lsoa_merge

# Spatial join listings by room type and lsoa, summarise by count
for r in ["Shared room", "Private room", "Entire home/apt"]:
    sjoin_lsoa = gpd.sjoin(listings[listings["room_type"] == r], lsoa_merge)
    count_dict = (
        sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
    )  # count the values with value counts
    lsoa_merge[r + "_count"] = lsoa_merge["LSOA21CD"].map(
        count_dict
    )  # map it back to lsoa_merge

# Change NA to 0
lsoa_merge = lsoa_merge.fillna(0)
```

```{python}
# Make the correlation matrix
lsoa_merge_sub1 = lsoa_merge.iloc[:, -8:]
lsoa_merge_sub1_comatrix = lsoa_merge_sub1.corr()
# plot the correlation matrix plot
fig, axes = plt.subplots(figsize=(20, 12))
sns.heatmap(
    lsoa_merge_sub1_comatrix,
    cmap="viridis",
    annot=True,
    fmt=".3f",
    linewidths=0.1,
    annot_kws={"size": 15},
    ax=axes,
)
axes.tick_params(labelsize=9)
axes.set_title(
    "Correlation Matrix - Listings type vs Crime severity density", fontsize=20
)
plt.show()
```

```{python}
# # Plot crime / listings
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 24))
# lsoa_merge.plot(ax=ax1, column="crime_count", legend=False, cmap="Reds", alpha=0.9)
# lsoa_merge.plot(ax=ax2, column="listing_count", legend=False, cmap="Blues", alpha=0.9)

# for ax in fig.get_axes():
#     ax.axes.xaxis.set_visible(False)
#     ax.axes.yaxis.set_visible(False)
#     ax.set_xlim([501000, 563000])
#     ax.set_ylim([155000, 202000])

# ax1.set_title("Crime count by LSOA, Sep 2023")
# ax2.set_title("Airbnb Listing count by LSOA, Sep 2023")
```

It shows stronger relationship between Entire home vs. Cirme density, followed by private room and then shared room. This is because bla bla.

##### 3. Visualising correlation on a map (not visible on PDF file, please view the HTML file on github)

Below is a bivariate choropleth map showing 2 variables. We see that 

- Obvious observation:...
- Not obvious observation:...
- How this informs our subsequent analyses:...

```{python}
# # Convert shapefil to geojson that the visualisation function requires
# lsoa_merge_wgs = lsoa_merge.to_crs(4326)
# lsoa_merge_wgs.to_file(os.path.join("data", "lsoa_merge.geojson"))
# geojson = json.load(open(os.path.join("data", "lsoa_merge.geojson"), "r"))
# # Iterate over JSON to add necessary 'id' property
# for i in range(len(geojson["features"])):
#     geojson["features"][i]["id"] = geojson["features"][i]["properties"]["LSOA21CD"]
```

```{python}
# # Load conf defaults and override some variables for our London map
# conf = conf_defaults()
# conf["plot_title"] = "Airbnb and Crime in London, Sep 2023"
# conf["hover_x_label"] = "Crime"  # Label to appear on hover
# conf["hover_y_label"] = "AirBnb listings"  # Label to appear on hover
# conf["width"] = 1000
# conf["center_lat"] = 51.508  # Latitude of the center of the map
# conf["center_lon"] = -0.1  # Longitude of the center of the map
# conf["map_zoom"] = 9  # Zoom factor of the map
# conf["borders_width"] = 0  # Width of the geographic entity borders

# # Define settings for the legend
# conf["top"] = 0.1  # Vertical position of the top right corner (0: bottom, 1: top)
# conf["right"] = 0.1  # Horizontal position of the top right corner (0: left, 1: right)
# conf["line_width"] = 0  # Width of the rectangles' borders
# conf["legend_x_label"] = "More Crime"  # x variable label for the legend
# conf["legend_y_label"] = "More Airbnb"  # y variable label for the legend

# # Plot
# fig = create_bivariate_map(
#     lsoa_merge,
#     color_sets["blue-orange"],
#     geojson,
#     x="crime_count",
#     y="listing_count",
#     ids="LSOA21CD",
#     name="LSOA21NM",
#     conf=conf,
# )
# fig.show()
```

![Airbnb vs Crime](https://www.dropbox.com/scl/fi/e8epq7uc943zvjz2es6si/newplot-1.png?rlkey=rlgydu69vjzuxnuvch8ptu2y1&dl=1)

```{python}
# # Load conf defaults and override some variables for our London map
# conf = conf_defaults()
# conf["plot_title"] = "Entire-home and Property Crime in London, Sep 2023"
# conf["hover_x_label"] = "Property Crime"  # Label to appear on hover
# conf["hover_y_label"] = "Entire home listings"  # Label to appear on hover
# conf["width"] = 1000
# conf["center_lat"] = 51.508  # Latitude of the center of the map
# conf["center_lon"] = -0.1  # Longitude of the center of the map
# conf["map_zoom"] = 9  # Zoom factor of the map
# conf["borders_width"] = 0  # Width of the geographic entity borders

# # Define settings for the legend
# conf["top"] = 0.1  # Vertical position of the top right corner (0: bottom, 1: top)
# conf["right"] = 0.1  # Horizontal position of the top right corner (0: left, 1: right)
# conf["line_width"] = 0  # Width of the rectangles' borders
# conf["legend_x_label"] = "More Crime"  # x variable label for the legend
# conf["legend_y_label"] = "More Airbnb"  # y variable label for the legend

# # Plot
# fig = create_bivariate_map(
#     lsoa_merge,
#     color_sets["blue-orange"],
#     geojson,
#     x="Property crime_count",
#     y="Entire home/apt_count",
#     ids="LSOA21CD",
#     name="LSOA21NM",
#     conf=conf,
# )
# fig.show()
```

![Entire home vs Property Crime](https://www.dropbox.com/scl/fi/2rgbdp0f9rfgs1n6dt6i0/newplot-2.png?rlkey=8zr6iippnd7n2v1oa7oncz9y5&dl=1)

- Concentration in the inner core. Not surprising
- Many areas with high airbnb with low crime (skyblue) in the centre -> correlation not clear
- Pockets 

##### Connection to #7:

Correaltion doesn't mean causation bla bla, therefore we would like to run a regression. We will pay extra attention to Entire-home etc. because of the high correlation seen above.

## 7.

\[Regression explanation of all the dependent and independent variables\]

Chosen columns: X

-   Population density: Usual resident (11) / Area (need to get)
-   Bad+Very Bad Health (120+121) / Pop (11)
-   deprived 2+ dimension (144+145+146) / Pop (11)
-   Unemployed (72) / Pop (11)
-   Young people (17+18+19) / Pop (11)
-   3x Airbnb listings (3 types) / Household number (3)

Y

-   Crime-relevant / Pop (11)

[Regression Data]

```{python}
{}
```

```{python}
{}
```

\[Regression codes\]

```{python}
{}
```

Comment

\[Refinement\]

```{python}
{}
```

Comment

\[Results\]

```{python}
{}
```

Comment

\[Discussion\]

```{python}
{}
```

Comment

\[Policy recommendation\] TBD. Some ideas (NEED LITERATURE TO BACKUP)

If there are relationships

1.  Reactive: Increase police presence in areas with high Airbnb density
2.  Proactive: Limit Airbnb density in areas with high crime rates of a certain type
3.  Collaborative: Airbnb to share data with local police departments to help them identify areas with high Airbnb density
4.  Punitive: Airbnb to pay a fine for every crime incident that occurs in areas with high Airbnb density
5.  Preventive: Airbnb to pay for additional security cameras in areas with high Airbnb density

If not: Other cities this but London doesn't see the same issue. Should look at improving crime by improving other aspects...

## References

Add reference
