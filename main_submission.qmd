---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: CHEER-UP's FSDS Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=20mm
      - left=20mm
      - right=20mm
      - bottom=20mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, CHEER UP Group, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used, we ensure that we have made its contribution to the final submission clear.

Date: 18 Dec 2023

Student Numbers: 23221457, 22172388, 23092079, 23252871, 23066570

Project Github: https://github.com/hanukikanker/fsds-group-cheers


## Brief Group Reflection

| What Went Well | What Was Challenging |
|----------------|----------------------|
| Insight from data analysis: Identifying that entire homes positively impact property crime provides valuable insight to inform targeted solutions.| Data Limitations: The difficulty in detailing whether crimes occurred within specific Airbnb listings due to data limitations poses a challenge in formulating precise recommendations.                    |
| Policy recommendations: We provided comprehensive recommendations for the government, Airbnb and users, covering various aspects, including background checks, regulations, security measures, and public awareness, demonstrating a holistic approach to addressing the challenges.           | Potential Causation: Why certain areas exhibited higher crime levels, e.g. a higher density of Airbnb rentals, specifically entire homes/apartments, may attract more tourists. This, in turn, could render these areas more susceptible to criminal activities, as tourists are often perceived as attractive targets for theft or scams.|
|Group dynamics: We distributed the tasks among the group but regrouped regularly to discuss our findings and the next steps. | Difficult in collaboration and prioritisation leading up to exam week.|

## Priorities for Feedback
Correctness of our quantitative methods and cleanness of codes

{{< pagebreak >}}

```{python}
#| jupyter: {source_hidden: true}
import warnings

warnings.filterwarnings("ignore")
import json
import math
import os
from urllib.parse import urlparse

import geopandas as gpd
import libpysal as ps
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.graph_objs as go
import pysal
import seaborn as sns
import statsmodels.api as sm
from esda.moran import Moran, Moran_Local
from libpysal.weights import Queen
from matplotlib import image as mpimg
from mgwr.gwr import GWR, MGWR
from mgwr.sel_bw import Sel_BW
from requests import get
from splot.esda import lisa_cluster
from spreg import GM_Error_Het
```

```{python}
#| jupyter: {source_hidden: true}
# Function to download file from remote
# Adapted from CASA0013


def cache_data(src: str, dest: str) -> str:
    """
    Create a folder to store file from URL.
    If folder doesn't already exists, then create one, before writing the file.

    src : URL
    dest : location on local drive
    """
    url = urlparse(src)  # We assume that this is some kind of valid URL
    fn = os.path.split(url.path)[-1]  # Extract the filename
    dfn = os.path.join(dest, fn)  # Destination filename as path

    if not os.path.isfile(dfn):
        print(f"{dfn} not found, downloading!")
        path = os.path.split(dest)
        if len(path) >= 1 and path[0] != "":
            os.makedirs(os.path.join(*path), exist_ok=True)
        with open(dfn, "wb") as file:
            response = get(src)
            file.write(response.content)
        print("\tDone downloading...")
    else:
        print(f"Found {dfn} locally!")
    return dfn
```

```{python}
#| jupyter: {source_hidden: true}
# Function to create bivariate chropleth map
# Adapted from: https://www.kaggle.com/code/yotkadata/bivariate-choropleth-map-using-plotly
"""
Function to set default variables
"""


def conf_defaults():
    # Define some variables for later use
    conf = {
        "plot_title": "Bivariate choropleth map using Ploty",  # Title text
        "plot_title_size": 20,  # Font size of the title
        "width": 1000,  # Width of the final map container
        "ratio": 0.8,  # Ratio of height to width
        "center_lat": 0,  # Latitude of the center of the map
        "center_lon": 0,  # Longitude of the center of the map
        "map_zoom": 3,  # Zoom factor of the map
        "hover_x_label": "Label x variable",  # Label to appear on hover
        "hover_y_label": "Label y variable",  # Label to appear on hover
        "borders_width": 0.5,  # Width of the geographic entity borders
        "borders_color": "#f8f8f8",  # Color of the geographic entity borders
        # Define settings for the legend
        "top": 1,  # Vertical position of the top right corner (0: bottom, 1: top)
        "right": 1,  # Horizontal position of the top right corner (0: left, 1: right)
        "box_w": 0.04,  # Width of each rectangle
        "box_h": 0.04,  # Height of each rectangle
        "line_color": "#f8f8f8",  # Color of the rectagles' borders
        "line_width": 0,  # Width of the rectagles' borders
        "legend_x_label": "Higher x value",  # x variable label for the legend
        "legend_y_label": "Higher y value",  # y variable label for the legend
        "legend_font_size": 9,  # Legend font size
        "legend_font_color": "#333",  # Legend font color
    }
    # Calculate height
    conf["height"] = conf["width"] * conf["ratio"]

    return conf


"""
Function to recalculate values in case width is changed
"""


def recalc_vars(new_width, variables, conf=conf_defaults()):
    # Calculate the factor of the changed width
    factor = new_width / 1000

    # Apply factor to all variables that have been passed to the function
    for var in variables:
        if var == "map_zoom":
            # Calculate the zoom factor
            # Mapbox zoom is based on a log scale. map_zoom needs to be set
            # to value ideal for our map at 1000px.
            # So factor = 2 ^ (zoom - map_zoom) and zoom = log(factor) / log(2) + map_zoom
            conf[var] = math.log(factor) / math.log(2) + conf[var]
        else:
            conf[var] = conf[var] * factor

    return conf


"""
Function that assigns a value (x) to one of three bins (0, 1, 2).
The break points for the bins can be defined by break_1 and break_2.
"""


def set_interval_value(x, break_1, break_2):
    if x <= break_1:
        return 0
    elif break_1 < x <= break_2:
        return 1
    else:
        return 2


"""
Function that adds a column 'biv_bins' to the dataframe containing the 
position in the 9-color matrix for the bivariate colors
    
Arguments:
    df: Dataframe
    x: Name of the column containing values of the first variable
    y: Name of the column containing values of the second variable

"""


def prepare_df(df, x="x", y="y"):
    # Check if arguments match all requirements
    if df[x].shape[0] != df[y].shape[0]:
        raise ValueError(
            "ERROR: The list of x and y coordinates must have the same length."
        )

    # Calculate break points at percentiles 33 and 66
    x_breaks = np.percentile(df[x], [33, 66])
    y_breaks = np.percentile(df[y], [33, 66])

    # Assign values of both variables to one of three bins (0, 1, 2)
    x_bins = [
        set_interval_value(value_x, x_breaks[0], x_breaks[1]) for value_x in df[x]
    ]
    y_bins = [
        set_interval_value(value_y, y_breaks[0], y_breaks[1]) for value_y in df[y]
    ]

    # Calculate the position of each x/y value pair in the 9-color matrix of bivariate colors
    df["biv_bins"] = [
        int(value_x + 3 * value_y) for value_x, value_y in zip(x_bins, y_bins)
    ]

    return df


"""
Function to create a color square containig the 9 colors to be used as a legend
"""


def create_legend(fig, colors, conf=conf_defaults()):
    # Reverse the order of colors
    legend_colors = colors[:]
    legend_colors.reverse()

    # Calculate coordinates for all nine rectangles
    coord = []

    # Adapt height to ratio to get squares
    width = conf["box_w"]
    height = conf["box_h"] / conf["ratio"]

    # Start looping through rows and columns to calculate corners the squares
    for row in range(1, 4):
        for col in range(1, 4):
            coord.append(
                {
                    "x0": round(conf["right"] - (col - 1) * width, 4),
                    "y0": round(conf["top"] - (row - 1) * height, 4),
                    "x1": round(conf["right"] - col * width, 4),
                    "y1": round(conf["top"] - row * height, 4),
                }
            )

    # Create shapes (rectangles)
    for i, value in enumerate(coord):
        # Add rectangle
        fig.add_shape(
            go.layout.Shape(
                type="rect",
                fillcolor=legend_colors[i],
                line=dict(
                    color=conf["line_color"],
                    width=conf["line_width"],
                ),
                xref="paper",
                yref="paper",
                xanchor="right",
                yanchor="top",
                x0=coord[i]["x0"],
                y0=coord[i]["y0"],
                x1=coord[i]["x1"],
                y1=coord[i]["y1"],
            )
        )

        # Add text for first variable
        fig.add_annotation(
            xref="paper",
            yref="paper",
            xanchor="left",
            yanchor="top",
            x=coord[8]["x1"],
            y=coord[8]["y1"],
            showarrow=False,
            text=conf["legend_x_label"] + " 🠒",
            font=dict(
                color=conf["legend_font_color"],
                size=conf["legend_font_size"],
            ),
            borderpad=0,
        )

        # Add text for second variable
        fig.add_annotation(
            xref="paper",
            yref="paper",
            xanchor="right",
            yanchor="bottom",
            x=coord[8]["x1"],
            y=coord[8]["y1"],
            showarrow=False,
            text=conf["legend_y_label"] + " 🠒",
            font=dict(
                color=conf["legend_font_color"],
                size=conf["legend_font_size"],
            ),
            textangle=270,
            borderpad=0,
        )

    return fig


"""
Function to create the map

Arguments:
    df: The dataframe that contains all the necessary columns
    colors: List of 9 blended colors
    x: Name of the column that contains values of first variable (defaults to 'x')
    y: Name of the column that contains values of second variable (defaults to 'y')
    ids: Name of the column that contains ids that connect the data to the GeoJSON (defaults to 'id')
    name: Name of the column conatining the geographic entity to be displayed as a description (defaults to 'name')
"""


def create_bivariate_map(
    df, colors, geojson, x="x", y="y", ids="id", name="name", conf=conf_defaults()
):
    if len(colors) != 9:
        raise ValueError(
            "ERROR: The list of bivariate colors must have a length eaqual to 9."
        )

    # Recalculate values if width differs from default
    if not conf["width"] == 1000:
        conf = recalc_vars(
            conf["width"],
            ["height", "plot_title_size", "legend_font_size", "map_zoom"],
            conf,
        )

    # Prepare the dataframe with the necessary information for our bivariate map
    df_plot = prepare_df(df, x, y)

    # Create the figure
    fig = go.Figure(
        go.Choroplethmapbox(
            geojson=geojson,
            locations=df_plot[ids],
            z=df_plot["biv_bins"],
            marker_line_width=0.5,
            colorscale=[
                [0 / 8, colors[0]],
                [1 / 8, colors[1]],
                [2 / 8, colors[2]],
                [3 / 8, colors[3]],
                [4 / 8, colors[4]],
                [5 / 8, colors[5]],
                [6 / 8, colors[6]],
                [7 / 8, colors[7]],
                [8 / 8, colors[8]],
            ],
            customdata=df_plot[
                [name, ids, x, y]
            ],  # Add data to be used in hovertemplate
            hovertemplate="<br>".join(
                [  # Data to be displayed on hover
                    "<b>%{customdata[0]}</b> (ID: %{customdata[1]})",
                    conf["hover_x_label"] + ": %{customdata[2]}",
                    conf["hover_y_label"] + ": %{customdata[3]}",
                    "<extra></extra>",  # Remove secondary information
                ]
            ),
        )
    )

    # Add some more details
    fig.update_layout(
        title=dict(
            text=conf["plot_title"],
            font=dict(
                size=conf["plot_title_size"],
            ),
        ),
        mapbox_style="white-bg",
        width=conf["width"],
        height=conf["height"],
        autosize=True,
        mapbox=dict(
            center=dict(
                lat=conf["center_lat"], lon=conf["center_lon"]
            ),  # Set map center
            zoom=conf["map_zoom"],  # Set zoom
        ),
    )

    fig.update_traces(
        marker_line_width=conf[
            "borders_width"
        ],  # Width of the geographic entity borders
        marker_line_color=conf[
            "borders_color"
        ],  # Color of the geographic entity borders
        showscale=False,  # Hide the colorscale
    )

    # Add the legend
    fig = create_legend(fig, colors, conf)

    return fig


# Define sets of 9 colors to be used
# Order: bottom-left, bottom-center, bottom-right, center-left, center-center, center-right, top-left, top-center, top-right
color_sets = {
    "pink-blue": [
        "#e8e8e8",
        "#ace4e4",
        "#5ac8c8",
        "#dfb0d6",
        "#a5add3",
        "#5698b9",
        "#be64ac",
        "#8c62aa",
        "#3b4994",
    ],
    "teal-red": [
        "#e8e8e8",
        "#e4acac",
        "#c85a5a",
        "#b0d5df",
        "#ad9ea5",
        "#985356",
        "#64acbe",
        "#627f8c",
        "#574249",
    ],
    "blue-orange": [
        "#fef1e4",
        "#fab186",
        "#f3742d",
        "#97d0e7",
        "#b0988c",
        "#ab5f37",
        "#18aee5",
        "#407b8f",
        "#5c473d",
    ],
}
```

```{python}
#| jupyter: {source_hidden: true}
# calculating VIF
# This function is adjusted from: https://stackoverflow.com/a/51329496/4667568
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant


def drop_column_using_vif_(df, thresh=5):
    """
    Calculates VIF each feature in a pandas dataframe, and repeatedly drop the columns with the highest VIF
    A constant must be added to variance_inflation_factor or the results will be incorrect

    :param df: the pandas dataframe containing only the predictor features, not the response variable
    :param thresh: (default 5) the threshould VIF value. If the VIF of a variable is greater than thresh, it should be removed from the dataframe
    :return: dataframe with multicollinear features removed
    """
    while True:
        # adding a constatnt item to the data. add_constant is a function from statsmodels (see the import above)
        df_with_const = add_constant(df)

        vif_df = pd.Series(
            [
                variance_inflation_factor(df_with_const.values, i)
                for i in range(df_with_const.shape[1])
            ],
            name="VIF",
            index=df_with_const.columns,
        ).to_frame()

        # drop the const
        vif_df = vif_df.drop("const")

        # if the largest VIF is above the thresh, remove a variable with the largest VIF
        # If there are multiple variabels with VIF>thresh, only one of them is removed. This is because we want to keep as many variables as possible
        if vif_df.VIF.max() > thresh:
            # If there are multiple variables with the maximum VIF, choose the first one
            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]
            print("Dropping: {}".format(index_to_drop))
            df = df.drop(columns=index_to_drop)
        else:
            # No VIF is above threshold. Exit the loop
            break

    return df
```

```{python}
#| jupyter: {source_hidden: true}
# Define paths for download
ddir = os.path.join("data", "geo")  # destination directory
spath = "https://github.com/jreades/i2p/blob/master/data/src/"  # source path
spath_db = "https://www.dropbox.com/scl/fi/"  # source path for dropbox
host = "http://orca.casa.ucl.ac.uk"
path = "~jreades/data"
```

```{python}
#| output: false
#| jupyter: {source_hidden: true}

# Read in airbnb, crime, and lsoa stats files. https://www.dropbox.com/home/casa/casa_fsds_cheers.
# Some files have been cleaned, cropped, turned into gdf and hosted on dropbox to save time later on. See cleandata.ipynb in the Github repo for more details on how these files were created.
# # [PATIENCE] Takes about 4 minutes to download and read in the data

# Crime data filtered for September 2023, within Greater London
crime = gpd.read_file(
    cache_data(
        spath_db
        + "ewc9skqcfr930whzcjadd/crime.gpkg?rlkey=xjve2mmmmt5h3allsa2zpabya&dl=1",
        ddir,
    ),
    driver="gpkg",
    low_memory=False,
)

# Airbnb listing data scraped on September 2023, filtered within Greater London
listings = gpd.read_parquet(
    cache_data(f"{host}/{path}/2023-09-06-listings.geoparquet", ddir)
)
listings = listings.to_crs(epsg=27700)

# LSOA (London) data concatenated from multiple datasets from London Datastore,
lsoa_stat = pd.read_csv(
    cache_data(
        spath_db
        + "8zc0g09rlf1yhj6j7yn6n/lsoa_full.csv?rlkey=yopr4hv017rr3iv4e2zus0ifz&dl=1",
        ddir,
    ),
    low_memory=False,
)
```

```{python}
#| output: false
#| jupyter: {source_hidden: true}

# Read in shapefiles (2 mins)
water = gpd.read_file(cache_data(spath + "Water.gpkg?raw=true", ddir))
green = gpd.read_file(cache_data(spath + "Greenspace.gpkg?raw=true", ddir))
boros = gpd.read_file(cache_data(spath + "Boroughs.gpkg?raw=true", ddir))
lsoa = gpd.read_file(
    cache_data(
        spath_db
        + "u367zlorie8vuluugy2fr/lsoa_london.gpkg?rlkey=rc7rdnlfdmzfgy5q7ujz9pnwj&dl=1",
        ddir,
    )
)
```

```{python}
#| output: false
#| jupyter: {source_hidden: true}

# Check that all geo dataframes that they are in EPSG:27700
crs = []
for n in [water, green, boros, lsoa, crime, listings]:
    crs.append(n.crs)
print(f"Geo dataframes are in {set(crs)}")
```

# Response to Questions

## 1. Who collected the data?

Inside Airbnb was founded by Murray Cox, who conceived the project and keeps updating and analysing Airbnb data (*About Inside Airbnb*, nd).

## 2. Why did they collect it?

Murray Cox and Co. aim to make Airbnb's data more transparent and accessible to the public. In addition to creating visual dashboards of different cities, they wrote many reports based on these data. For example, they criticised Airbnb’s New York data as misleading (Cox and Slee, et al. 2016). This is meant to benefit others in different ways:

- Consumers can understand market trends more efficiently, which help them make more informed decisions and choose accommodation that fits their needs and budget;
- Landlords can have transparency of information, and that can promote fair competition and maintain the stability of short-term rental market prices;
- Academics can use these data to explore how Airbnb relates to many factors in urban development.
- Governments can understand the current situation of the short-term rental market and try to control the phenomenon that could be more conducive to the stable development of the city by establishing more supervision regulations.

## 3. How was the data collected?

Inside Airbnb uses web scraping scripts to extract publicly available information from Airbnb's website every quarter. These scripts navigate the Airbnb site, accessing various pages to retrieve data such as listing titles, descriptions, hosts, pricing, and more. The collected data is verified, cleaned, and made available on the Inside Airbnb website for users to explore online or download for analysis. In addition to providing a snapshot, Inside Airbnb conducts in-depth investigations using this raw data to derive more insightful information for users. For instance, the platform employs a proprietary "San Francisco Model" to estimate the frequency with which an Airbnb listing is rented out and to approximate a listing's income.

## 4. How does the method of collection impact the accuracy of its representation of the process it seeks to study, and what broader issues does this raise?

Several potential issues with this method of data collection may challenge the validity of any research work done based on it:

- Pricing and availability, among others, are highly dynamic attributes set by hosts that cannot be captured accurately with web-scraping, which only reflects a snapshot of the website at one specific moment. Data capture can only capture real-time Airbnb data, and long-term comparative analysis requires data collection in advance and at higher frequencies
- Data quality is susceptible to changes in the website's structure. Web scraping also only accesses publicly available listings.
- Data quality depends on user input and does not guarantee accuracy (e.g. bogus listings)
- Has ethical and legal implications, which will be discussed in the next section

## 5. What ethical considerations does the use of this data raise?

- First, landlords and renters have the right to request that their personal information not be made public elsewhere. Therefore, using this data may be needed to handle sensitive personal information and consider hosts' privacy and home security.
- Second, it may cause data misuse to affect public opinion (D’Ignazio and Klein, 2020). For example, the rental activities of a particular group may be negatively analysed, reinforcing the stereotype of this group and then triggering discrimination and antagonism toward society.
- Third, power always influences data settings (D’Ignazio and Klein, 2020). Airbnb often leaves out reviews that aren’t friendly to properties and focuses on those that are profitable, such as setting its recommendation algorithm to favour partners.

## 6. With reference to the data, what does an analysis of Listing types suggest about the nature of Airbnb lets in London?

#### Background and research question
As a short-term rental platform, the mobility of tenants is one of its major characteristics. Mobility is a factor in crime (Mburu and Helbich, 2016), so recently there has been increasing concern about whether Airbnb, which brings high mobility to the local community, could be linked to an increase or decrease in crime. For example, Xu, Pennington-Gray and Kim (2019) studied the relationship between Airbnb density and crime in Florida and found that certain types of listings do have a significant impact on crime. Therefore, we try to explore whether Airbnb in London could also have an impact on crime based on Lower Layer Super Output Areas (LSOAs) level. The reason why we choose LSOA level as this allows for a more detailed examination of localized patterns and variations within a borough. Different neighborhoods within a borough can exhibit distinct characteristics that might be overlooked in a broader borough-level analysis as crime rates and housing patterns can vary significantly from one neighborhood to another. 

In short, our research question is: Does the type of Airbnb listing have an impact on crime in the neighbourhood? 

#### Data source
The Airbnb dataset is from the September 2023 updated version provided by Inside Airbnb. The crime dataset is from data.police.uk and has the same time dimension as Airbnb’s. The spatial partitioning dimension in this study is LSOA and is from the Office of National Statistics website. In order to examine the nature of short-lets and crime relationship, we want to first establish the parameters to focus on just a subset. As Xu, Pennington-Gray, and Kim (2019) mentioned different types of short-lets have different impacts, we refer to their classification standards defining them as follows (*Data Dictionary*, nd):  
- _Entire home:_ Tenants can enjoy an entire whole listing.  
- _Private room:_ The tenant has a private bedroom but may need to share some space with others such as the kitchen.  

We **excluded Hotel rooms** (listings by actual hotels and hostels) since they are not the subject of this research and are also few in numbers. We also **exclude Shared rooms** due to thier limited number in London. Lastly, we trimmed listing price outliers to filter out bogus listings. Figure 1 shows the listing price distribution of different room types before filtering.

```{python}
# Transform price to float
if listings.price.dtype == "O":
    listings.price = (
        listings.price.str.replace("$", "").str.replace(",", "").astype(float).round(2)
    )
# Trim outliers in terms of price based on interquartile range, because they are most likely they are not real listings
q1 = listings.price.quantile(0.25)
q3 = listings.price.quantile(0.75)
iqr = q3 - q1
listings1 = listings[
    (listings.price >= q1 - 1.5 * iqr) & (listings.price <= q3 + 1.5 * iqr)
]
```

```{python}
# Plot by room type
room = listings.room_type.unique()
palette = sns.color_palette()

fig, ax1 = plt.subplots(figsize=(7, 3))
for color, r in zip(palette, room):
    ax1.hist(
        listings1[listings1["room_type"] == r].price,
        bins=50,
        alpha=1,
        label=r,
        color=color,
    )
ax1.legend(loc="upper right")
fig.suptitle(
    "Figure 1: Histogram of Airbnb listing price by room type - Sept 2023",
    x=0.5,
    y=-0.01,
)
plt.tight_layout()
plt.show()
```

```{python}
# Clean listings data: outliers, and missing values, split into 2 types by property type
# Omit Hotel rooms and Shared rooms
listings2 = listings1[~listings1["room_type"].isin(["Hotel room", "Shared room"])]
```

```{python}
# Write it back to original dataframe
listings = listings2.copy()
```

As for crime data, as we wanted to make recommendations more specific to different categories of crime, we classified crime into the following types (Xu, Pennington-Gray and Kim, 2019; Flatley, 2016):
- **Public order:** public order, drugs, possession of weapons, anti-social behaviour.
- **Violent (involve force):** robbery, violence and sexual offences.
- **Property (without force):** bicycle theft, burglary, criminal damage and arson, shoplifting, theft from the person, vehicle crime..

These three types of crime will be treated as dependent variables. Figure 2 shows the distribution of crime density per LSOA by type. Note that they take on a skewed distribution (Poisson distribution) for being based on count. We will treat this accordingly in our analysis while also not trimming any outlying values at this stage.

```{python}
# Clean crime data: outliers, and missing values, split into 3 types
crime_property = [
    "Burglary",
    "Criminal damage and arson",
    "Theft from the person",
    "Vehicle crime",
    "Shoplifting",
    "Bicycle theft",
]
crime_violent = ["Robbery", "Violence and sexual offences"]
crime_publicorder = [
    "Anti-social behaviour",
    "Public order",
    "Drugs",
    "Possession of weapons",
]
# Omit rows with Other Crime and Other Theft
crime = crime[~crime["Crime type"].isin(["Other theft", "Other crime"])]
# Create a new column for crime type
crime["Category"] = np.where(
    crime["Crime type"].isin(crime_property),
    "Property crime",
    np.where(
        crime["Crime type"].isin(crime_violent), "Violent crime", "Public Order crime"
    ),
)
```

```{python}
# Plot them
fig, ax = plt.subplots(1, 3, figsize=[8, 3], sharey=True)
for idx, r in enumerate(crime.Category.unique()):
    ax[idx].hist(
        crime[crime["Category"] == r].groupby("LSOA code").count().Category,
        bins=100,
        alpha=0.5,
        label=r,
    )
    ax[idx].legend(loc="upper right")
fig.suptitle("Figure 2: Histogram of each crime category (LSOA level)", x=0.5, y=0)
plt.show()
```

#### Relationship between Airbnb listings and crime

```{python}
lsoa_merge = lsoa.copy()

# Spatial crime and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(crime, lsoa_merge)
count_dict = (
    sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
)  # count the values with value counts
lsoa_merge["crime_count"] = lsoa_merge["LSOA21CD"].map(
    count_dict
)  # map it back to lsoa

# Spatial join listings and lsoa, summarise by count
sjoin_lsoa = gpd.sjoin(listings, lsoa_merge)
count_dict = (
    sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
)  # count the values with value counts
lsoa_merge["listing_count"] = lsoa_merge["LSOA21CD"].map(
    count_dict
)  # map it back to lsoa
```

```{python}
# Spatial join crime by categories and lsoa, summarise by count
for s in ["Public Order crime", "Property crime", "Violent crime"]:
    sjoin_lsoa = gpd.sjoin(crime[crime["Category"] == s], lsoa_merge)
    count_dict = (
        sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
    )  # count the values with value counts
    lsoa_merge[s + "_count"] = lsoa_merge["LSOA21CD"].map(
        count_dict
    )  # map it back to lsoa_merge

# Spatial join listings by room type and lsoa, summarise by count
for r in ["Private room", "Entire home/apt"]:
    sjoin_lsoa = gpd.sjoin(listings[listings["room_type"] == r], lsoa_merge)
    count_dict = (
        sjoin_lsoa["LSOA21CD"].value_counts().to_dict()
    )  # count the values with value counts
    lsoa_merge[r + "_count"] = lsoa_merge["LSOA21CD"].map(
        count_dict
    )  # map it back to lsoa_merge

# Change NA to 0
lsoa_merge = lsoa_merge.fillna(0)
```

```{python}
# Plot crime / listings by type
fig, axes = plt.subplots(3, 2, figsize=(7, 10))

lsoa_merge.plot(
    ax=axes[0, 0], column="Public Order crime_count", cmap="Reds", alpha=0.9
)
lsoa_merge.plot(ax=axes[1, 0], column="Property crime_count", cmap="Reds", alpha=0.9)
lsoa_merge.plot(ax=axes[2, 0], column="Violent crime_count", cmap="Reds", alpha=0.9)

axes[0, 0].set_title("Public Order crime by LSOA", fontsize=8)
axes[1, 0].set_title("Property crime by LSOA", fontsize=8)
axes[2, 0].set_title("Violent crime by LSOA", fontsize=8)

axes[0, 1].axis("off")
lsoa_merge.plot(ax=axes[1, 1], column="Private room_count", cmap="Blues", alpha=0.9)
lsoa_merge.plot(ax=axes[2, 1], column="Entire home/apt_count", cmap="Blues", alpha=0.9)

axes[1, 1].set_title("Private room by LSOA", fontsize=8)
axes[2, 1].set_title("Entire home/apt by LSOA", fontsize=8)

for ax in fig.get_axes():
    ax.axis("off")
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)
    ax.set_xlim([501000, 563000])
    ax.set_ylim([155000, 202000])

plt.tight_layout()
fig.suptitle("Figure 3: Shortlet and crime density by LSOA", x=0.5, y=0)
plt.show()
```

##### 1. Spatial distribution
To start our spatial analysis of crime and listing patterns across LSOAs, distinct geographic concentrations emerge. Public order crimes are dispersed citywide, with notable pockets in Westminster and Hillingdon. Property crimes concentrate in central London, particularly in Westminster, while violent crimes exhibit a similar pattern, with Westminster and Hillingdon standing out. On the other hand, private rooms and entire homes exhibit citywide dispersal, with Westminster and Camden featuring prominently in Private room listing concentrations and the City of London emerging as a hub for entire homes. 

##### 2. Global Correlation and Spatial correlation
Zooming in to compare these variables among LSOAs via a correlation matrix with our variables of interest: We see that the correlation between **entire home** and **property crime** is the strongest (0.51). On the contrary, the correlation between private rooms and property crime is the weakest (0.284). Overall, it is interesting to see that entire home listings seem to be more exposed to crime, reinforcing the perception among opponents of Airbnb that not having live-in either engenders or is a magnet for crime. 

```{python}
# Make the correlation matrix
cols = [col for col in lsoa_merge.columns if "count" in col]

lsoa_merge_sub1 = lsoa_merge[cols]
lsoa_merge_sub1_comatrix = lsoa_merge_sub1.corr()
# plot the correlation matrix plot
fig, axes = plt.subplots(figsize=(7, 5))
sns.heatmap(
    lsoa_merge_sub1_comatrix,
    cmap="viridis",
    annot=True,
    fmt=".3f",
    linewidths=0.1,
    annot_kws={"size": 8},
    ax=axes,
)
axes.tick_params(labelsize=9)

fig.suptitle(
    "Figure 4: Correlation Matrix - Listing type vs Crime category", x=0.5, y=-0.03
)
plt.tight_layout()
plt.show()
```

Beyond static correlation, when visualised on a map, more patterns emerge. Other notable areas outside of central London with a high concentration of crime incidents and Airbnb listings are **Hillington (Heathrow Airport), Richmond upon Thames, Croydon, Greenwich, Hounslow, etc.** Some are tourist destinations, while others, in contrast, seem to be in residential areas. This might allude to an underlying relationship between the concentration of Airbnb and crimes with tourism in general or population centres, and not between these variables alone. 

Based on descriptive statistics so far, we can draw certain mixed conclusions about the relationship between crime and short-let. On the one hand, the distribution of Airbnb listings does align with the distribution of crime in many areas. More notably, we find correlations between Entire home/apt and Property crime. On the other hand, an underlying factor might shape this relationship across space (spatial dependence, confounding variables such as tourism attractiveness, etc.) 

```{python}
# Convert shapefile to geojson that the visualisation function requires
lsoa_merge_wgs = lsoa_merge.to_crs(4326)
lsoa_merge_wgs.to_file(os.path.join("data", "lsoa_merge.geojson"))
geojson = json.load(open(os.path.join("data", "lsoa_merge.geojson"), "r"))
# Iterate over JSON to add necessary 'id' property
for i in range(len(geojson["features"])):
    geojson["features"][i]["id"] = geojson["features"][i]["properties"]["LSOA21CD"]
```

```{python}
# Load conf defaults and override some variables for our London map
conf = conf_defaults()
conf["plot_title"] = "Short-lets and Crime in London 2023"
conf["hover_x_label"] = "Crime"  # Label to appear on hover
conf["hover_y_label"] = "AirBnb listings"  # Label to appear on hover
conf["width"] = 700
conf["center_lat"] = 51.508  # Latitude of the center of the map
conf["center_lon"] = -0.1  # Longitude of the center of the map
conf["map_zoom"] = 9  # Zoom factor of the map
conf["borders_width"] = 0  # Width of the geographic entity borders

# Define settings for the legend
conf["top"] = 0.1  # Vertical position of the top right corner (0: bottom, 1: top)
conf["right"] = 0.1  # Horizontal position of the top right corner (0: left, 1: right)
conf["line_width"] = 0  # Width of the rectangles' borders
conf["legend_x_label"] = "More Crime"  # x variable label for the legend
conf["legend_y_label"] = "More Airbnb"  # y variable label for the legend

# Plot
fig = create_bivariate_map(
    lsoa_merge,
    color_sets["blue-orange"],
    geojson,
    x="crime_count",
    y="listing_count",
    ids="LSOA21CD",
    name="LSOA21NM",
    conf=conf,
)
fig.show()
```

```{python}
#| output: false
img = cache_data(
    "https://www.dropbox.com/scl/fi/e8epq7uc943zvjz2es6si/newplot-1.png?rlkey=rlgydu69vjzuxnuvch8ptu2y1&dl=1",
    ddir,
)
```

```{python}
plt.figure(figsize=(7, 15))
plt.title(
    "Figure 5: Bivariate Map of Airbnb and Crime in London*\n(Interactive version available on the HTML output)",
    x=0.5,
    y=-0.1,
)
image = mpimg.imread(os.path.join(ddir, "newplot-1.png"))
plt.imshow(image)
plt.axis("off")
plt.tight_layout()
plt.show()
```

## 7. Shortlet vs Crime and Policy recommendations

#### Preparing Regression Data

We will perform a regression analysis to determine whether different shortlet types has an effect on crime overall and also different crime categories, controlling for other relevant sociodemographic variables as much as possible. These control variables directly or indirectly provide a level of deprivation, which serves as a significant socioeconomic indicator linked to crime patterns (*Crimes recorded by neighbourhood income deprivation decile in London*, 2022). 

The target dependent variables are :
- proportion of all crime, property crime, public order crime, and violent crime, over LSOA population

The target indendpendent variables (predictors) are:
- proportion of Private and Entire home listings over total household

The control independent variables are:
- population density,
- proportion of unemployed people among 16+
- proportion of young people (15-24 ages),
- proportion of household with a deprivation index of 3+
- proportion of people with qualification level 3 or higher
- proportion of tenanted household over all households

Since we are using a simple linear regression to model this relationship to test our hypothesis, we must make sure the variables satisfy basic distribution requirements. Therefore we performed log transformations for select variable. Figure 7 shows the variables post-transformation.

```{python}
# Select columns and calculate density for key variables
lsoa_stat_sel = lsoa_stat.copy()
lsoa_stat_sel["deprived_3+_dimension_density"] = (
    lsoa_stat["3 dimensions"] + lsoa_stat["4 dimensions"]
) / lsoa_stat["All Households"]
lsoa_stat_sel["unemployed_density"] = (
    lsoa_stat["Economically active: Unemployed"]
) / lsoa_stat["All usual residents aged 16 or over"]
lsoa_stat_sel["young_people_density"] = (
    lsoa_stat["Aged 15 "]
    + lsoa_stat["Aged 16 to 17"]
    + lsoa_stat["Aged 18 to 19"]
    + lsoa_stat["Aged 20 to 24"]
) / lsoa_stat["All usual residents"]
lsoa_stat_sel["qual_level_3+_density"] = (
    lsoa_stat["Level 3"] + lsoa_stat["Level 4+"]
) / lsoa_stat["Usual residents aged 16+"]
lsoa_stat_sel["social_rent_density"] = (
    lsoa_stat["Other social rented"] + lsoa_stat["Rented from Local Authority"]
) / lsoa_stat["All Households"]
```

```{python}
# Merge selected columns from lsoa_stat_sel to lsoa_merge by LSOA21CD
lsoa_merge1 = lsoa_merge.merge(
    lsoa_stat_sel[
        [
            "LSOA code",
            "deprived_3+_dimension_density",
            "unemployed_density",
            "young_people_density",
            "qual_level_3+_density",
            "social_rent_density",
            "All Households",
            "All usual residents aged 16 or over",
            "All usual residents",
        ]
    ],
    left_on="LSOA21CD",
    right_on="LSOA code",
    how="left",
).drop(columns=["LSOA code"])
```

```{python}
# Add density for each column in lsoa_merge1 nased on All Households and All usual residents
# Select columns and calculate density
lsoa_merge1["crime_density"] = (
    lsoa_merge1["crime_count"] / lsoa_merge1["All usual residents"]
)
lsoa_merge1["listing_density"] = (
    lsoa_merge1["listing_count"] / lsoa_merge1["All Households"]
)
lsoa_merge1["Private room_density"] = (
    lsoa_merge1["Private room_count"] / lsoa_merge1["All Households"]
)
lsoa_merge1["Entire home/apt_density"] = (
    lsoa_merge1["Entire home/apt_count"] / lsoa_merge1["All Households"]
)
lsoa_merge1["Public Order crime_density"] = (
    lsoa_merge1["Public Order crime_count"] / lsoa_merge1["All usual residents"]
)
lsoa_merge1["Property crime_density"] = (
    lsoa_merge1["Property crime_count"] / lsoa_merge1["All usual residents"]
)
lsoa_merge1["Violent crime_density"] = (
    lsoa_merge1["Violent crime_count"] / lsoa_merge1["All usual residents"]
)
lsoa_merge1["area"] = lsoa_merge1["geometry"].area
lsoa_merge1["pop_density"] = lsoa_merge1["All usual residents"] / lsoa_merge1["area"]
```

```{python}
regressiondata = lsoa_merge1.copy()
# Keep ones needed for regression
regressiondata = regressiondata.loc[
    :, [col for col in regressiondata.columns if "density" in col]
]
```

```{python}
# Function to approximate what a normal distribution would be from a dataset
# Adapted from CASA0013


def normal_from_dist(
    series,
):  # define function name and required arguments (in this case a pandas series)
    mu = series.mean()  # calculate the mean of our data
    sd = series.std()  # calculate the standard deviation of our data
    n = len(series)  # count how many observations are in our data
    s = np.random.normal(
        mu, sd, n
    )  # use the parameters of the data just calculated to generate n random numbers, drawn from a normal distributions
    return s  # return this set of random numbers
```

```{python}
# test_cols_df = regressiondata.copy()

# fig, axes = plt.subplots(5,3, figsize=(7, 10))
# axflat = axes.flatten()
# for i, ax in enumerate(np.resize(axflat,axflat.size-2)):
#     sns.distplot(test_cols_df.iloc[:,i], ax=ax)
#     sns.kdeplot(normal_from_dist(test_cols_df.iloc[:,i]), color='r', fill=True, ax=ax)
#     ax.set_title(test_cols_df.columns[i],fontsize=7)
#     ax.set_xlabel('')
#     ax.set_ylabel('')
#     ax.set_yticklabels('')
#     sns.despine()
# for ax in axflat[-2:]:
#     ax.axis('off')
# fig.suptitle('Figure 6: Regression dependent and indenpendent variables (pre-transformation)',x=0.5,y=0)
# plt.tight_layout()
# plt.show()
```

```{python}
# Log of the variables social rent, private room, entire home/apt, pop_density, etc.
cols = [
    "social_rent_density",
    "Private room_density",
    "Entire home/apt_density",
    "pop_density",
    "listing_density",
    "crime_density",
    "listing_density",
    "Public Order crime_density",
    "Property crime_density",
    "Violent crime_density",
]
for col in cols:
    # remove rows (LSOAs) with no airbnbs
    c = regressiondata[col].sort_values().unique()[1]
    regressiondata[col + "_log"] = np.log(regressiondata[col] + c)
# Drop original columns
regressiondata.drop(cols, axis=1, inplace=True)
```

```{python}
test_cols_df = regressiondata.copy()

fig, axes = plt.subplots(5, 3, figsize=(7, 7))
axflat = axes.flatten()
for i, ax in enumerate(np.resize(axflat, axflat.size - 2)):
    sns.distplot(test_cols_df.iloc[:, i], ax=ax)
    sns.kdeplot(normal_from_dist(test_cols_df.iloc[:, i]), color="r", fill=True, ax=ax)
    ax.set_title(test_cols_df.columns[i], fontsize=7)
    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_yticklabels("")
    sns.despine()
for ax in axflat[-2:]:
    ax.axis("off")
fig.suptitle(
    "Figure 6: Regression dependent and indenpendent variables (post-log-transformation)",
    x=0.5,
    y=0,
)
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

# drop crime related columns to check for multicollinearity
regressiondata_VIF = regressiondata.drop(
    [col for col in regressiondata.columns if "crime" in col], axis=1
)
# drop columns with high VIF
regressiondata_VIF = drop_column_using_vif_(regressiondata_VIF)
selected_cols = regressiondata_VIF.columns.tolist()
```

#### Regression Results

We found some key predictor variables to be statistically significant, most notably within our target variables (All Regression Results can be found in the Appendix):

- **Entire home/apt** has a notable effect on property crime incidents, followed by public order crime and, finally, violent crime.
- Meanwhile, **Private room** interestingly only has a notable and significant effect on public order crime and violent crime. 

```{python}
models = []
results = []
for i in ["crime_", "Property crime_", "Violent crime_", "Public Order crime_"]:
    # OLS model
    OLS = sm.OLS(
        endog=regressiondata[i + "density_log"],
        exog=sm.add_constant(regressiondata[selected_cols]),
    )
    OLSmodel = OLS.fit()
    models.append(OLS)
    results.append(OLSmodel)
    # print(OLSmodel.summary())
    # print('='* 98)
    # print('\n')
```

Before we can confirm our hypothesis that there is an impact of increased short-let presence on different types of crime, we will need to first consider the fact that the crime observed in an LSOA affects crime observed in the neighbouring LSOAs results in spatial autocorrelation among the residuals. In fact, Moran's I test stats in Figure 7 show that, as a whole, there is only a slight spatial autocorrelation among the residuals (Coeffs close to 0). Areas where our model might under-predict crime are in red (HH), whereas areas where our model might over-predict crime are in blue (LL). 

Fortunately, the Spatial Error Model, which introduced a spatial component into our original OLS model for All Crimes (see in Appendix), shows that it does not significantly affect the coefficients of our target variables. More specifically:

- **Private room** has a slightly stronger effect when spatial errors are added. (coeff=0.043 vs 0.040)
- **Entire home/apt** has a slightly weaker effect when spatial errors are added. (coeff=0.1473 vs 0.1645)

With this, we can conclude that there is a positive effect of the density of Airbnb listings on crime in a neighbourhood, especially for Entire home/apt listings.

```{python}
# Local Moran's I for residual of OLSmodel

# Create spatial weights matrix
w = Queen.from_dataframe(lsoa)
w.transform = "r"

residuals = results[0].resid
moran = Moran(residuals, w)
moran_loc = Moran_Local(residuals, w)
lisa_cluster(
    moran_loc, lsoa, p=0.05, figsize=(7, 5), legend_kwds={"loc": "lower right"}
)
plt.title(f"Residuals Moran's I - {models[0].endog_names}\nGlobal stat = {moran.I:.2f}")
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 3, figsize=(7, 3))
ax = axes.flatten()
for i in range(3):
    residuals = results[i].resid
    moran = Moran(residuals, w)
    moran_loc = Moran_Local(residuals, w)
    lisa_cluster(moran_loc, lsoa, p=0.05, ax=ax[i], legend=False)
    ax[i].set_title(
        f"Residuals Local Moran's I - {models[i+1].endog_names}\nGlobal stat = {moran.I:.2f}",
        fontsize=5,
    )

fig.suptitle("Figure 7: Residual Morans I - by crime category", x=0.5, y=0.1)
plt.tight_layout()
plt.show()
```

#### Do short-lets in different areas have a different effect on crime?

Using a Geographically Weighted Regression, we can further visualise where precisely the effect of short lets on crime is statistically significant. The results show that:

- **Private Room** presence has a particularly strong effect on crime (including property crime) in East and Southeast London (City, Boroughs of Tower Hamlets, Bermondsey, Southwark, etc.)
- **Entire Home** presence has a particularly strong effect on crime (including Property Crime) in Central London (Boroughs of Westminster, Camden, South Islington)

```{python}
# Prepare dataset inputs - Crime
g_y = regressiondata["crime_density_log"].values.reshape((-1, 1))
g_X = regressiondata[selected_cols].values
lsoa_cen = lsoa.centroid.get_coordinates()
u = lsoa_cen["x"]
v = lsoa_cen["y"]
g_coords = list(zip(u, v))

g_X = (g_X - g_X.mean(axis=0)) / g_X.std(axis=0)
g_y = (g_y - g_y.mean(axis=0)) / g_y.std(axis=0)

# Calibrate GWR model
# gwr_selector = Sel_BW(g_coords, g_y, g_X)
# gwr_bw = gwr_selector.search(bw_min=2)
# print(gwr_bw)
gwr_bw = 522  # I calibrated this
gwr_results = GWR(g_coords, g_y, g_X, gwr_bw).fit()

# Add Paramaters back to geo dataframe for mapping
lsoa["gwr_intercept"] = gwr_results.params[:, 0]
for i in range(len(selected_cols) - 1):
    lsoa["gwr_" + selected_cols[i]] = gwr_results.params[:, i + 1]

# Filter Insignificant coefficients alpha 0.05
gwr_filtered_t = gwr_results.filter_tvals(alpha=0.05)
```

```{python}
# Prepare dataset inputs - Property Crime
g_y1 = regressiondata["Property crime_density_log"].values.reshape((-1, 1))
g_X = regressiondata[selected_cols].values
lsoa_cen = lsoa.centroid.get_coordinates()
u = lsoa_cen["x"]
v = lsoa_cen["y"]
g_coords = list(zip(u, v))

g_X = (g_X - g_X.mean(axis=0)) / g_X.std(axis=0)
g_y1 = (g_y1 - g_y1.mean(axis=0)) / g_y1.std(axis=0)

# Calibrate GWR model
# gwr_selector1 = Sel_BW(g_coords, g_y1, g_X)
# gwr_bw1 = gwr_selector1.search(bw_min=2)
# print(gwr_bw1)
gwr_bw1 = 560  # I calibrated this
gwr_results1 = GWR(g_coords, g_y1, g_X, gwr_bw1).fit()

# Add Paramaters back to geo dataframe for mapping
lsoa["gwr_intercept_prop_"] = gwr_results1.params[:, 0]
for i in range(len(selected_cols) - 1):
    lsoa["gwr_prop_" + selected_cols[i]] = gwr_results1.params[:, i + 1]

# Filter Insignificant coefficients alpha 0.05
gwr_prop_filtered_t = gwr_results1.filter_tvals(alpha=0.05)
```

```{python}
# Map them both, sig coeff only
# Adapted from https://deepnote.com/@carlos-mendez/PYTHON-GWR-and-MGWR-71dd8ba9-a3ea-4d28-9b20-41cc8a282b7a
import matplotlib.colors as mcolors

type = ["Private room", "Entire home/apt"]
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))
for i in range(len(type)):
    # All crime
    lsoa.plot(
        column=f"gwr_{type[i]}_density_log",
        cmap="RdBu_r",
        linewidth=0.001,
        scheme="naturalbreaks",
        k=4,
        legend=True,
        legend_kwds={"bbox_to_anchor": (1.10, 0.2)},
        ax=axes[i, 0],
    )
    lsoa[gwr_filtered_t[:, i + 6] == 0].plot(
        color="white", linewidth=0.001, edgecolor="white", ax=axes[i, 0]
    )  # 6th variables in variable array
    boros.plot(edgecolor="red", facecolor="None", linewidth=0.1, ax=axes[i, 0])

    # Property crime
    lsoa.plot(
        column=f"gwr_prop_{type[i]}_density_log",
        cmap="RdBu_r",
        linewidth=0.001,
        scheme="naturalbreaks",
        k=4,
        legend=True,
        legend_kwds={"bbox_to_anchor": (1.10, 0.2)},
        ax=axes[i, 1],
    )
    lsoa[gwr_prop_filtered_t[:, i + 6] == 0].plot(
        color="white", linewidth=0.001, edgecolor="white", ax=axes[i, 1]
    )  # 6th variables in variable array
    boros.plot(edgecolor="red", facecolor="None", linewidth=0.1, ax=axes[i, 1])

    axes[i, 0].axis("off")
    axes[i, 1].axis("off")
    axes[i, 0].set_title(
        f"({i+1}a) {type[i]} vs. Crime, significant coeffs", fontsize=8
    )
    axes[i, 1].set_title(
        f"({i+1}b) {type[i]} vs. Property Crime, significant coeffs", fontsize=8
    )
plt.tight_layout()
fig.suptitle(
    "Geographically weighted effect of vacation homes on crime", x=0.5, y=0
)
plt.show()
```

#### Reflection and Recommendations
After the public consultation on the policy in 2022, the government will require landlords to register and will conduct random checks on some listings or require landlords to upload legal documents to prove the safety of their listings (Consultation on a registration scheme for short-term lets in England, 2023). While this will allow for better listing management and tenants' safety, we recommend further actions based on our analysis thus far:

First, for **property crime**, *entire home* shortlets are at particular risk, especially in central London, and *private room* listings in East London, whether shortlet tenants are committing theft or disturbances against neighbours or being targeted. We suggest that Airbnb should strengthen the background checks of tenants (Barron, Kung, and Proserpio, 2018) and choose whether to ban the use of the Airbnb platform according to the number of tenants with criminal property records. If permitted, Airbnb needs to provide relevant information to the landlord and keep records synchronised with local police departments. In the property security protection of tenants, Airbnb needs to strengthen the door lock management, such as encouraging the landlord to replace the smart door lock as much as possible and allowing each group of tenants to reset their password.

Second, there is an impact of shortlets on **violent crime and public order crime**. Since our crime data cannot be detailed into whether or not the offence occurred within the listing, the following recommendations will encompass all stakeholders:

* **a) For Airbnb**, more regulations on tenants' behaviour are needed. For example, while Airbnb currently states the required quiet time for tenants on its booking interface, it does not specify penalties. For tenants reported for taking drugs, disturbing residents, committing crimes against neighbours or listings, or violating Airbnb’s rules, Airbnb shall block the tenant’s Airbnb account based on the severity. Moreover, Airbnb should set up efficient channels to ensure responsive security measures, making it easier for tenants or landlords to file complaints against each other. Especially after receiving safety reports about the listings, Airbnb should consider taking the listings offline (Grind and Shifflett, 2019). 
* **b) For the government**, there are benefits to strictly regulating shortlets, e.g. enforcing owner-occupied residences, a cap on the number of renters, and neighbour notifications (Vande Bunte, 2014). Moreover, we could refer to San Francisco’s approach, which imposes strict rules on the duration each landlord can rent out a property each year, effectively controlling the number of short-rental properties (Cromarty, 2023), reducing the transient population (Mburu and Helbich, 2016). Besides, a conscious effort should be made to increase police presence in places where Airbnb is particularly dense. 
* **c) For tenants**, both Airbnb and the government should encourage them to understand and comply with local laws and regulations regarding short-term rentals like the New York State Multiple Dwelling Law (Schneiderman, 2014) through various channels, such as news pop-ups and educational publicity programs. Improving public awareness is essential to reducing crime and creating a safe environment (Lemieux and Felson, 2019).

## Appendix:

#### 1. OLS Regression Results

--------------------------------------------

OLS Regression results for each of the four dependent variables:

```{python}
# Regression Results

for i in results:
    print(i.summary())
    print("\n")
```

*Note:* our log transformation has greatly treated the Heteroscedasticity of the variables, although some do remain without greatly affecting the OLS regression results.

```{python}
# Residuals vs. Fitted plot (All Crime model)
fig, axes = plt.subplots(1, 3, figsize=(7, 2.5))
# Plot the residuals vs fitted values on ax1
axes[0].scatter(results[0].fittedvalues, results[0].resid, alpha=0.5, s=5)
axes[0].set_xlabel(f"Fitted values")
axes[0].set_ylabel("Residual")
axes[0].set_title(f"Residual vs. Fitted Plot for {models[0].endog_names}", fontsize=7)

# QQplot on ax2
sm.qqplot(results[0].resid, line="s", markersize=1, ax=axes[1])
axes[1].set_title(f"QQ Plot for {models[0].endog_names}", fontsize=7)

# Residual distribution on ax3
axes[2].hist(results[0].resid, bins=50)
axes[2].set_xlabel("Residual")
axes[2].set_ylabel("Frequency")
axes[2].set_title(f"Residual Distribution for {models[0].endog_names}", fontsize=7)

fig.suptitle(
    "Figure 9: Residual Analysis to confirm OLS assumptions (All Crime)", x=0.5, y=0
)
plt.tight_layout()
plt.show()
```

#### 2. Spatial Error Model Results

```{python}
# Spatial Error Model to deal with spatial autocorrelation

X = regressiondata[selected_cols].to_numpy()
y = regressiondata["crime_density_log"].to_numpy()

# Create a Queen spatial weights matrix
w = Queen.from_dataframe(lsoa)
w.transform = "r"

mod1 = GM_Error_Het(
    y=y,
    x=X,
    w=w,
    name_y="crime_density_log",
    name_x=regressiondata[selected_cols].columns.tolist(),
)
print(mod1.summary)
```

## References

*About Inside Airbnb* (nd) Available at: http://insideairbnb.com/about/ (Accessed: 5 December 2023).

Barron, K., Kung, E. and Proserpio, D. (2018) * The Sharing Economy and Housing
Affordability: Evidence from Airbnb*. Available at: https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf (Accessed: 14 December 2023).

*Consultation on a registration scheme for short-term lets in England* (2023) Available at: https://www.gov.uk/government/consultations/consultation-on-a-registration-scheme-for-short-term-lets-in-england/consultation-on-a-registration-scheme-for-short-term-lets-in-england (Accessed: 5 December 2023).

Cox, M. and Slee, T. (2016) *How Airbnb’s data hid the facts in New York City*. Available at: http://insideairbnb.com/research/how-airbnb-hid-the-facts-in-nyc (Accessed: 14 December 2023).

*Crime and income deprivation* (nd) Available at: https://trustforlondon.org.uk/data/crime-and-income-deprivation/#:~:text=Overall%2C%2052%25%20more%20crimes%20were,the%20least%20income%2Ddeprived%2010%25 (Accessed: 14 December 2023).

Cromarty, H. (2023) *The growth in short-term lettings (England)*. Available at: https://researchbriefings.files.parliament.uk/documents/CBP-8395/CBP-8395.pdf (Accessed: 18 December 2023).

D’Ignazio, C. and Klein, L.F. (2020) *The Numbers Don’t Speak for Themselves’*. United States: MIT Libraries Experimental Collections Fund.

Flatley, J. (2016) *Focus on property crime: year ending March 2016*. Available at: https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/bulletins/focusonpropertycrime/yearendingmarch2016 (Accessed: 14 December 2023).

Grind, K. and Shifflett, S. (2019) ‘Shooting, Sex Crime and Theft: Airbnb Takes Halting Steps to Protect Its Users’, *The Wall Street Journal*. (Accessed: 17 December 2023).

*Inside Airbnb Data Dictionary* (nd) Available at: https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?usp=sharing (Accessed: 17 December 2023).

Lemieux, A.M. and Felson, M. (2019) ‘Tourist and Visitor Crime’, in Natarajan, M. (ed.) *International and Transnational Crime and Justice*. Cambridge University Press.

Mburu, L.W. and Helbich, M. (2016) ‘Crime Risk Estimation with a Commuter-Harmonized Ambient Population’, *Annals of the American Association of Geographers*, 106(4), pp. 804-818.

Schneiderman, E. (2014) *Airbnb in the city. New York State Office of the Attorney General*. Available at: http://www.ag.ny.gov/pdfs/Airbnbreport.pdf (Accessed 14 December 2023).

Vande Bunte, M. (2014) *Airbnb verdict in Grand Rapids: 1 room and 2 adults, with a possible exception*. Available at: https://www.mlive.com/news/grand-rapids/2014/08/airbnb_verdict_in_grand_rapids.html (Accessed: 17 December 2023).

Xu, Y.H., Kim, J. and Pennington-Gray, L. (2019) ‘The Sharing Economy: A Geographically Weighted Regression Approach to Examine Crime and the Shared Lodging Sector’, *Journal of Travel Research*, 58(7), pp. 1193-1208. Available at: https://journals.sagepub.com/doi/10.1177/0047287518797197 (Accessed: 17 December 2023).

Occasional use of Github Copilot AI within Visual Studio Code IDE
